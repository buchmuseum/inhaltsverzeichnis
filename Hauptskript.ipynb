{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==========================================================\n",
    "Titel:    DNB-SRU-Analyse und Schlagwortmatching mit DDC\n",
    "Autor:    [Isabell Sickert]\n",
    "Datum:    [16.10.2025]\n",
    "Version:  2.0\n",
    "==========================================================\n",
    "\n",
    "Beschreibung:\n",
    "Dieses Skript dient der automatisierten Abfrage, Anreicherung und Analyse\n",
    "von Titeldaten der Deutschen Nationalbibliothek (DNB) anhand der SRU-Schnittstelle.\n",
    "Es kombiniert bibliografische Metadaten mit Schlagwort- und DDC-Informationen,\n",
    "führt eine Volltextsuche in vorhandenen Inhaltsverzeichnissen durch und erstellt eine strukturierte,\n",
    "gewichtete Ergebnistabelle in Excel-Form.\n",
    "\n",
    "----------------------------------------------------------\n",
    "Ablauf und Hauptfunktionen:\n",
    "----------------------------------------------------------\n",
    "\n",
    "1) Schlagworte und DDC-Mapping einlesen:\n",
    "   - Liest „idn.xlsx“ (Liste von DNB-IDNs) und\n",
    "     „schlagworte_ddc_gewichtung.xlsx“ (Schlagwort-Mapping).\n",
    "   - Erstellt daraus:\n",
    "       • `keywords` → Liste aller Schlagwörter zur Textsuche\n",
    "       • `weights` → Schlagwort-Gewichtungen (Relevanz)\n",
    "       • `systematics` → Zuordnung Schlagwort → Systematik\n",
    "\n",
    "2) SRU-Abfrage (query_dnb):\n",
    "   - Für jede IDN wird die DNB-SRU-Schnittstelle aufgerufen.\n",
    "   - Extrahiert bibliografische Informationen:\n",
    "       • Titel, Zusatztitel, Autor, Verlag, Jahr\n",
    "       • DDC-Codes (Fachklassifikation)\n",
    "       • Sachgruppen (alternative Klassifikation)\n",
    "       • GND-Schlagwörter (Personen- und Geoschlagwörter)\n",
    "   - Erkennt, ob der Titel im DBSM-Bestand vorhanden ist.\n",
    "   - Kürzt DDC-Codes automatisch bis zur ersten Null nach dem Punkt.\n",
    "   - Schreibt alle Ergebnisse zurück in das Haupt-DataFrame.\n",
    "\n",
    "3) DDC-Kürzungsfunktion:\n",
    "   - Funktion `shorten_ddc_cell` säubert und kürzt DDC-Werte in Spalten.\n",
    "   - Wird auf „DDC1“ und „DDC2“ angewendet, um saubere Klassifikationen\n",
    "     für spätere Auswertungen zu erhalten.\n",
    "\n",
    "4) Inhaltsverzeichnisse durchsuchen (find_keywords):\n",
    "   - Ruft den Volltext eines DNB-Titels ab (`/04/text`).\n",
    "   - Durchsucht den Text nach vordefinierten Schlagwörtern.\n",
    "   - Zählt Treffer, berechnet die Gesamtgewichtung und bestimmt\n",
    "     die am besten passende Systematik (höchste Relevanz).\n",
    "   - Gibt zusätzlich Links zum Volltext und PDF zurück.\n",
    "\n",
    "5) Schlagwort- und DDC-Mapping (apply_mapping):\n",
    "   - Durchsucht zusätzlich Titel und Zusatztitel nach bekannten Begriffen.\n",
    "   - Erhöht ggf. die Gesamtgewichtung und ergänzt Systematiken.\n",
    "   - Dient der Feinjustierung des Klassifikationsergebnisses.\n",
    "\n",
    "6) Filtern (aktuell deaktiviert):\n",
    "   - Optional können Titel nach Gewichtung, DDC-Blacklist,\n",
    "     Verlagen oder Inhalt (z. B. „Roman“) herausgefiltert werden.\n",
    "   - Diese Filter sind vorbereitet, aber derzeit auskommentiert.\n",
    "\n",
    "7) Export & Formatierung:\n",
    "   - Speichert die Resultate als „neu_test2.xlsx“.\n",
    "   - Erstellt klickbare Hyperlinks für Text- und PDF-URLs.\n",
    "   - Ausgabe: Eine Excel-Tabelle mit allen relevanten\n",
    "     Metadaten, Klassifikationen und Schlagworttreffern.\n",
    "\n",
    "----------------------------------------------------------\n",
    "Abhängigkeiten:\n",
    "- pandas\n",
    "- requests\n",
    "- BeautifulSoup4\n",
    "- openpyxl\n",
    "- xml.etree.ElementTree\n",
    "- re\n",
    "\n",
    "----------------------------------------------------------\n",
    "Ausgabe:\n",
    "→ Datei: „neu_test2.xlsx“\n",
    "Enthält alle analysierten DNB-Titel mit\n",
    "  - bibliografischen Daten,\n",
    "  - DDC- und Schlagwortzuordnungen,\n",
    "  - Relevanzgewichtung,\n",
    "  - Hyperlinks zu Text/PDF,\n",
    "  - DBSM-Kennzeichnung.\n",
    "\n",
    "==========================================================\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sickerti\\AppData\\Local\\Temp\\ipykernel_17296\\3338783985.py:188: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fertig! Ergebnisse in neu_test2.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "from openpyxl import load_workbook\n",
    "import re\n",
    "\n",
    "# ===============================\n",
    "# 1) Schlagworte + DDC-Mapping einlesen\n",
    "# ===============================\n",
    "idn_df = pd.read_excel(\"idn.xlsx\", dtype=str)\n",
    "\n",
    "mapping_df = pd.read_excel(\"schlagworte_ddc_gewichtung.xlsx\", dtype=str)\n",
    "keywords = mapping_df[mapping_df[\"Typ\"] == \"Schlagwort\"][\"Begriff\"].dropna().tolist()\n",
    "weights = mapping_df.set_index(\"Begriff\")[\"Gewichtung\"].astype(int).to_dict()\n",
    "systematics = mapping_df.set_index(\"Begriff\")[\"Systematik\"].to_dict()\n",
    "\n",
    "# ===============================\n",
    "# 2) SRU-Abfrage zuerst\n",
    "# ===============================\n",
    "def query_dnb(idn):\n",
    "    url = f\"https://services.dnb.de/sru/dnb?version=1.1&operation=searchRetrieve&query=idn={idn}&recordSchema=MARC21plus-xml\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return None, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "    root = ET.fromstring(response.text)\n",
    "    ns = {'marc': 'http://www.loc.gov/MARC21/slim'}\n",
    "\n",
    "    gnds1, gnds2 = [], []\n",
    "\n",
    "    title, subtitle, author, publisher, year = None, None, None, None, None\n",
    "    ddc_list, sachgruppe_list = [], []\n",
    "    subjects, geo_subjects = [], []\n",
    "\n",
    "    for record in root.findall(\".//marc:record\", ns):\n",
    "        for datafield in record.findall(\"marc:datafield\", ns):\n",
    "            tag = datafield.get(\"tag\")\n",
    "            ind1 = datafield.get(\"ind1\")\n",
    "\n",
    "            if tag == \"245\":\n",
    "                sub_a = datafield.find(\"marc:subfield[@code='a']\", ns)\n",
    "                sub_b = datafield.find(\"marc:subfield[@code='b']\", ns)\n",
    "                title = sub_a.text if sub_a is not None else None\n",
    "                subtitle = sub_b.text if sub_b is not None else None\n",
    "\n",
    "            elif tag == \"100\":\n",
    "                sub_a = datafield.find(\"marc:subfield[@code='a']\", ns)\n",
    "                author = sub_a.text if sub_a is not None else None\n",
    "\n",
    "            elif tag in [\"260\", \"264\"]:\n",
    "                sub_b = datafield.find(\"marc:subfield[@code='b']\", ns)\n",
    "                sub_c = datafield.find(\"marc:subfield[@code='c']\", ns)\n",
    "                publisher = sub_b.text if sub_b is not None else None\n",
    "                year = sub_c.text if sub_c is not None else None\n",
    "\n",
    "            elif tag == \"082\":\n",
    "                sub_a = datafield.find(\"marc:subfield[@code='a']\", ns)\n",
    "                if sub_a is not None:\n",
    "                    if ind1 == \"0\":  # echte DDC\n",
    "                        ddc_list.append(sub_a.text)\n",
    "                    elif ind1 == \"7\":  # Sachgruppe\n",
    "                        sachgruppe_list.append(sub_a.text)\n",
    "\n",
    "            elif tag == \"083\":\n",
    "                sub_a = datafield.find(\"marc:subfield[@code='a']\", ns)\n",
    "                if sub_a is not None:\n",
    "                    if ind1 == \"0\":  # echte DDC\n",
    "                        ddc_list.append(sub_a.text)\n",
    "                    elif ind1 == \"7\":  # Sachgruppe\n",
    "                        sachgruppe_list.append(sub_a.text)\n",
    "\n",
    "            elif tag == \"650\":  # GNDS1\n",
    "                for sub in datafield.findall(\"marc:subfield[@code='a']\", ns):\n",
    "                    if sub is not None:\n",
    "                        gnds1.append(sub.text)\n",
    "\n",
    "            elif tag == \"651\":  # GNDS2\n",
    "                for sub in datafield.findall(\"marc:subfield[@code='a']\", ns):\n",
    "                    if sub is not None:\n",
    "                        gnds2.append(sub.text)\n",
    "\n",
    "    # DBSM prüfen\n",
    "    dbsm_flag = None\n",
    "    dbsm_standort = None\n",
    "\n",
    "    for holding in root.findall(\".//marc:record[@type='Holdings']\", ns):\n",
    "        for datafield in holding.findall(\"marc:datafield[@tag='852']\", ns):\n",
    "            sub_b = datafield.find(\"marc:subfield[@code='b']\", ns)  # Standort\n",
    "            if sub_b is not None:\n",
    "                standort = sub_b.text\n",
    "                if \"dbsm\" in standort.lower():\n",
    "                    dbsm_flag = \"x\"\n",
    "                    dbsm_standort = standort  # Standort merken\n",
    "                    break\n",
    "\n",
    "    # DDC nur bis zur ersten Null nach dem Punkt kürzen\n",
    "    def shorten_ddc(ddc):\n",
    "        if not ddc:\n",
    "            return \"\"\n",
    "        parts = ddc.split(\".\")\n",
    "        if len(parts) == 2:\n",
    "            # alles nach der ersten Null nach dem Punkt abschneiden\n",
    "            match = re.match(r\"(\\d+\\.\\d*?)0\", ddc)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "            else:\n",
    "                return ddc\n",
    "        return ddc\n",
    "\n",
    "    ddc_cleaned = \"; \".join(shorten_ddc(d) for d in ddc_list if d)\n",
    "    ddc1 = ddc_cleaned.split(\";\")[0] if ddc_cleaned else \"\"\n",
    "    ddc2 = ddc_cleaned.split(\";\")[1] if len(ddc_cleaned.split(\";\")) > 1 else \"\"\n",
    "\n",
    "    return (\n",
    "        title or \"\",\n",
    "        subtitle or \"\",\n",
    "        author or \"\",\n",
    "        publisher or \"\",\n",
    "        year or \"\",\n",
    "        dbsm_flag or \"\",\n",
    "        dbsm_standort or \"\",\n",
    "        ddc1,\n",
    "        ddc2,\n",
    "        \"; \".join(sachgruppe_list) if sachgruppe_list else \"\",\n",
    "        \"; \".join(subjects) if subjects else \"\",\n",
    "        \"; \".join(geo_subjects) if geo_subjects else \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "for idx, row in idn_df.iterrows():\n",
    "    title, subtitle, author, publisher, year, dbsm_flag, dbsm_standort, ddc1, ddc2, sachgruppe, gnds1, gnds2 = query_dnb(row.iloc[0])\n",
    "    idn_df.at[idx, \"Titel\"] = title\n",
    "    idn_df.at[idx, \"Zusatztitel\"] = subtitle\n",
    "    idn_df.at[idx, \"Autor\"] = author\n",
    "    idn_df.at[idx, \"Verlag\"] = publisher\n",
    "    idn_df.at[idx, \"Jahr\"] = year\n",
    "    idn_df.at[idx, \"DBSM-Bestand\"] = dbsm_flag\n",
    "    idn_df.at[idx, \"DBSM-Standort\"] = dbsm_standort\n",
    "    idn_df.at[idx, \"DDC1\"] = ddc1\n",
    "    idn_df.at[idx, \"DDC2\"] = ddc2\n",
    "    idn_df.at[idx, \"Sachgruppe\"] = sachgruppe\n",
    "    idn_df.at[idx, \"GNDS1\"] = gnds1\n",
    "    idn_df.at[idx, \"GNDS2\"] = gnds2\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# DDC auf erste Nachkommastelle kürzen\n",
    "# ===============================\n",
    "import re\n",
    "\n",
    "def shorten_ddc_cell(cell_value):\n",
    "    \"\"\"Kürzt alle DDC-Codes in einer Zelle nach der ersten Null nach dem Punkt.\"\"\"\n",
    "    if pd.isna(cell_value) or not cell_value.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    parts = [part.strip() for part in cell_value.split(\";\")]\n",
    "    shortened_parts = []\n",
    "    for ddc in parts:\n",
    "        match = re.match(r\"(\\d+\\.\\d*?)0\", ddc)\n",
    "        if match:\n",
    "            shortened_parts.append(match.group(1))\n",
    "        else:\n",
    "            shortened_parts.append(ddc)\n",
    "    return \"; \".join(shortened_parts)\n",
    "\n",
    "# Beide Spalten anwenden\n",
    "for col in [\"DDC1\", \"DDC2\"]:\n",
    "    if col in idn_df.columns:\n",
    "        idn_df[col] = idn_df[col].apply(shorten_ddc_cell)\n",
    "\n",
    " \n",
    "\n",
    "# ===============================\n",
    "# 3) Inhaltsverzeichnisse durchsuchen\n",
    "# ===============================\n",
    "def find_keywords(idn):\n",
    "    url_text = f\"https://d-nb.info/{idn}/04/text\"\n",
    "    url_pdf = f\"https://d-nb.info/{idn}/04/pdf\"\n",
    "    formatted = \"\"  # <-- Initialisierung\n",
    "    total_weight = 0\n",
    "    best_syst = None\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url_text, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            text = soup.get_text().lower()\n",
    "\n",
    "            word_counts = {}\n",
    "            for word in keywords:\n",
    "                pattern = re.compile(rf\"\\b\\w*{re.escape(word.lower())}\\w*\\b\", re.IGNORECASE)\n",
    "                matches = pattern.findall(text)\n",
    "                count = len(matches)\n",
    "                if count > 0:\n",
    "                    word_counts[word] = (count, weights.get(word, 1), systematics.get(word, \"\"))\n",
    "\n",
    "            if word_counts:\n",
    "                total_weight = sum(v[1] for v in word_counts.values())\n",
    "                if len(word_counts) >= 3 or total_weight >= 3:\n",
    "                    best_word, (best_count, best_weight, best_syst) = max(\n",
    "                        word_counts.items(),\n",
    "                        key=lambda x: (x[1][1], x[1][0])\n",
    "                    )\n",
    "                    formatted = \"; \".join(\n",
    "                        f\"{w} (Gewicht={wt}, Treffer={ct})\"\n",
    "                        for w, (ct, wt, _) in word_counts.items()\n",
    "                    )\n",
    "\n",
    "    except requests.RequestException:\n",
    "        formatted = \"Fehler\"\n",
    "\n",
    "    return url_text or \"\", url_pdf or \"\", formatted or \"\", total_weight or 0, best_syst or \"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "idn_df[[\"URL\", \"PDF-URL\", \"Gefundene Schlagwörter\", \"Gesamtgewichtung\", \"Gewinner-Systematik\"]] = \\\n",
    "    idn_df.iloc[:, 0].apply(find_keywords).apply(pd.Series)\n",
    "\n",
    "idn_df.rename(columns={idn_df.columns[0]: \"IDN\"}, inplace=True)\n",
    "#idn_df = idn_df.dropna(subset=[\"Gefundene Schlagwörter\"])\n",
    "\n",
    "# ===============================\n",
    "# 4) Schlagwort- und DDC-Mapping\n",
    "# ===============================\n",
    "def apply_mapping(row):\n",
    "    total_weight = row[\"Gesamtgewichtung\"]\n",
    "    systematik = row[\"Gewinner-Systematik\"]\n",
    "    found_extra = []\n",
    "\n",
    "    # Titel + Zusatztitel durchsuchen\n",
    "    for text in [row[\"Titel\"], row[\"Zusatztitel\"]]:\n",
    "        if pd.notna(text):\n",
    "            t = text.lower()\n",
    "            for w in keywords:\n",
    "                if re.search(rf\"\\b{re.escape(w.lower())}\\b\", t):\n",
    "                    total_weight += weights.get(w, 1)\n",
    "                    found_extra.append(w)\n",
    "                    if not systematik:\n",
    "                        systematik = systematics.get(w)\n",
    "\n",
    "    # DDC prüfen\n",
    "    #if row[\"DDC\"]:\n",
    "        #for ddc_code in row[\"DDC\"].split(\";\"):\n",
    "            #ddc_code = ddc_code.strip()\n",
    "            #if not ddc_code:\n",
    "             #   continue\n",
    "            #if ddc_code in systematics:\n",
    "             #   systematik = systematics[ddc_code]\n",
    "              #  found_extra.append(f\"DDC {ddc_code}\")\n",
    "            #for key in systematics.keys():\n",
    "             #   if key.startswith(f\"{ddc_code} +\"):\n",
    "              #      systematik = systematics[key]\n",
    "               #     found_extra.append(key)\n",
    "\n",
    "    return pd.Series([total_weight, systematik, \"; \".join(found_extra)])\n",
    "\n",
    "\n",
    "idn_df[[\"Gesamtgewichtung\", \"Gewinner-Systematik\", \"Zusätzliche Treffer\"]] = \\\n",
    "    idn_df.apply(apply_mapping, axis=1)\n",
    "\n",
    "# ===============================\n",
    "# 5) Filtern\n",
    "# ===============================\n",
    "#idn_df = idn_df[idn_df[\"Gesamtgewichtung\"] >= 4]\n",
    "#idn_df = idn_df[~idn_df[\"Zusatztitel\"].str.contains(r\"\\broman\\b\", case=False, na=False)]\n",
    "\n",
    "#blacklist_ddc = [\"360\", \"590\", \"910\", \"333\", \"355\", \"370\", \"510\", \"610\", \"621\", \"690\", \"796\", \"797\", \"798\", \"799\", \"914\"]\n",
    "#idn_df = idn_df[~idn_df[\"DDC\"].str.startswith(tuple(blacklist_ddc), na=False)]\n",
    "\n",
    "#verlage_df = pd.read_excel(\"Verlage.xlsx\")\n",
    "#blacklist = verlage_df[\"Verlag\"].dropna().unique().tolist()\n",
    "#idn_df = idn_df[~idn_df[\"Verlag\"].isin(blacklist)]\n",
    "\n",
    "# ===============================\n",
    "# 6) Excel speichern + Hyperlinks\n",
    "# ===============================\n",
    "output_file = \"neu_test2.xlsx\"\n",
    "idn_df.to_excel(output_file, index=False)\n",
    "\n",
    "wb = load_workbook(output_file)\n",
    "ws = wb.active\n",
    "col_url = [c[0] for c in enumerate(ws[1]) if c[1].value == \"URL\"][0] + 1\n",
    "col_pdf = [c[0] for c in enumerate(ws[1]) if c[1].value == \"PDF-URL\"][0] + 1\n",
    "\n",
    "for row in range(2, ws.max_row + 1):\n",
    "    if ws.cell(row=row, column=col_url).value:\n",
    "        ws.cell(row=row, column=col_url).hyperlink = ws.cell(row=row, column=col_url).value\n",
    "        ws.cell(row=row, column=col_url).style = \"Hyperlink\"\n",
    "    if ws.cell(row=row, column=col_pdf).value:\n",
    "        ws.cell(row=row, column=col_pdf).hyperlink = ws.cell(row=row, column=col_pdf).value\n",
    "        ws.cell(row=row, column=col_pdf).style = \"Hyperlink\"\n",
    "\n",
    "wb.save(output_file)\n",
    "print(\"Fertig! Ergebnisse in neu_test2.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
